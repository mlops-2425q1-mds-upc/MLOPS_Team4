{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-28 11:12:56.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mconfig\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: D:\\Documents\\Data Science\\MLOPS\\MLOPS_Team4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from config import RAW_DATA_DIR, PROCESSED_DATA_DIR\n",
    "\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"sentiment\", \"id\", \"date\", \"query_string\", \"user\", \"text\"]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    RAW_DATA_DIR / \"training.1600000.processed.noemoticon.csv\",\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    encoding=\"latin-1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform date to month, day and hour\n",
    "month = []\n",
    "day = []\n",
    "hour = []\n",
    "\n",
    "for t in df[\"date\"]:\n",
    "    date = datetime.strptime(t, \"%a %b %d %H:%M:%S PDT %Y\")\n",
    "    month.append(date.month)\n",
    "    day.append(date.day)\n",
    "    hour.append(date.hour)\n",
    "\n",
    "df[\"month\"] = month\n",
    "df[\"day\"] = day\n",
    "df[\"hour\"] = hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable: Positive sentences are labelled as 1\n",
    "df[\"positive\"] = df[\"sentiment\"].replace([0, 4], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process sentences\n",
    "# extracted from https://pub.aimind.so/a-comprehensive-guide-to-text-preprocessing-for-twitter-data-getting-ready-for-sentiment-analysis-e7f91cd03671\n",
    "\n",
    "# Lowercase\n",
    "df['text_cleaned'] = df['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Removing punctuation\n",
    "import string\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Removing numbers\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "# Removing extra spaces\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# Replacing repetitions of punctuation\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: re.sub(r'(\\W)\\1+', r'\\1', x))\n",
    "\n",
    "# Removing special characters\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: re.sub(r\"[^\\w\\s]\", '', x))\n",
    "\n",
    "# Remove contractions from the 'text_cleaned' column\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "try:\n",
    "    df['tokens'] = df['text_cleaned'].apply(lambda x: word_tokenize(x))\n",
    "except LookupError: \n",
    "    import nltk\n",
    "    nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# POS tag mapping dictionary\n",
    "wordnet_map = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
    "\n",
    "# Function to perform Lemmatization on a text\n",
    "def lemmatize_text(text):\n",
    "    # Get the POS tags for the words\n",
    "    pos_tags = nltk.pos_tag(text)\n",
    "    \n",
    "    # Perform Lemmatization\n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos_tags:\n",
    "        # Map the POS tag to WordNet POS tag\n",
    "        pos = wordnet_map.get(tag[0].upper(), wordnet.NOUN)\n",
    "        # Lemmatize the word with the appropriate POS tag\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos=pos)\n",
    "        # Add the lemmatized word to the list\n",
    "        lemmatized_words.append(lemmatized_word)\n",
    "    \n",
    "    return lemmatized_words\n",
    "\n",
    "try:\n",
    "    # Apply Lemmatization to the 'tokens' column\n",
    "    df['tokens_lemm'] = df[\"tokens\"].apply(lemmatize_text)\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "    df['tokens_lemm'] = df[\"tokens\"].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df.drop(\"id\", \"query_string\", \"sentiment\", \"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "df.to_csv(str(PROCESSED_DATA_DIR) + \"preprocessed_dataset.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
