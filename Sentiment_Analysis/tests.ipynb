{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-30 15:01:49.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mconfig\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: D:\\Documents\\Data Science\\MLOPS\\MLOPS_Team4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from config import RAW_DATA_DIR, INTERIM_DATA_DIR, PARAMS_DIR\n",
    "\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import re\n",
    "import yaml\n",
    "from loguru import logger\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"sentiment\", \"id\", \"date\", \"query_string\", \"user\", \"text\"]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    RAW_DATA_DIR / \"training.1600000.processed.noemoticon.csv\",\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    encoding=\"latin-1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable: Positive sentences are labelled as 1\n",
    "df[\"positive\"] = df[\"sentiment\"].replace([0, 4], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df = df.drop(columns = [\"id\", \"query_string\", \"sentiment\", \"date\", \"user\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FX504\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning sentences\n",
    "# extracted from https://www.kaggle.com/code/arunrk7/nlp-beginner-text-classification-using-lstm\n",
    "# from Arun Pandian R \n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "text_cleaning_re = r\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stem=False):\n",
    "  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "  tokens = []\n",
    "  for token in text.split():\n",
    "    if token not in stop_words:\n",
    "      if stem:\n",
    "        tokens.append(stemmer.stem(token))\n",
    "      else:\n",
    "        tokens.append(token)\n",
    "  return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmarization, if it is indicated by the user\n",
    "with open(PARAMS_DIR, \"r\") as params_file:\n",
    "    try:\n",
    "        params = yaml.safe_load(params_file)\n",
    "        params = params[\"preprocessing\"]\n",
    "    except yaml.YAMLError as exc:\n",
    "            print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(lambda x: preprocess(x, stem=params[\"steam\"]))\n",
    "\n",
    "# extra cleaning \n",
    "# extracted from https://pub.aimind.so/a-comprehensive-guide-to-text-preprocessing-for-twitter-data-getting-ready-for-sentiment-analysis-e7f91cd03671\n",
    "\n",
    "# Removing numbers\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "# Remove contractions from the \"text\" column\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "logger.info(\"All sentences have been cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put target variable at the 1rst position\n",
    "df = df.reindex([\"positive\", \"text\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# POS tag mapping dictionary\n",
    "wordnet_map = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
    "\n",
    "# Function to perform Lemmatization on a text\n",
    "def lemmatize_text(text):\n",
    "    # Get the POS tags for the words\n",
    "    pos_tags = nltk.pos_tag(text)\n",
    "    \n",
    "    # Perform Lemmatization\n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos_tags:\n",
    "        # Map the POS tag to WordNet POS tag\n",
    "        pos = wordnet_map.get(tag[0].upper(), wordnet.NOUN)\n",
    "        # Lemmatize the word with the appropriate POS tag\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos=pos)\n",
    "        # Add the lemmatized word to the list\n",
    "        lemmatized_words.append(lemmatized_word)\n",
    "    \n",
    "    lemmatize_sentence = \" \".join(lemmatized_words)\n",
    "    return lemmatize_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"lemmarize\"] == True:\n",
    "\n",
    "    # Tokenize\n",
    "    try:\n",
    "        df['tokens'] = df[\"text\"].apply(lambda x: word_tokenize(x))\n",
    "    except LookupError: \n",
    "        import nltk\n",
    "        nltk.download('punkt_tab')\n",
    "        df['tokens'] = df[\"text\"].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "    try:\n",
    "        # Apply Lemmatization to the 'tokens' column\n",
    "        df['tokens'] = df[\"tokens\"].apply(lemmatize_text)\n",
    "    except LookupError:\n",
    "        nltk.download('averaged_perceptron_tagger_eng')\n",
    "        df['lemm_text'] = df[\"tokens\"].apply(lemmatize_text)\n",
    "\n",
    "    del df[\"tokens\"]\n",
    "    logger.info(\"Lemmarizer is appliyed to all sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "df.to_csv(str(INTERIM_DATA_DIR) + \"/\" + \"preprocessed_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import PROCESSED_DATA_DIR, PARAMS_DIR\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    PROCESSED_DATA_DIR / \"preprocessed_dataset.csv\"\n",
    ")\n",
    "\n",
    "with open(PARAMS_DIR, \"r\") as params_file:\n",
    "    try:\n",
    "        params = yaml.safe_load(params_file)\n",
    "        params = params[\"split\"]\n",
    "    except yaml.YAMLError as exc:\n",
    "            print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"positive\"]\n",
    "X = df.drop(columns='positive')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=params[\"test_size\"], train_size = params[\"train_size\"], random_state = params[\"random_state\"])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size= params[\"val_size\"], train_size = 1 - params[\"val_size\"], random_state = params[\"random_state\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
