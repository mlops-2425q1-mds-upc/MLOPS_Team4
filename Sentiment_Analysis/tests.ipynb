{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import RAW_DATA_DIR, PROCESSED_DATA_DIR, PARAMS_DIR\n",
    "\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import re\n",
    "import yaml\n",
    "from loguru import logger\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"sentiment\", \"id\", \"date\", \"query_string\", \"user\", \"text\"]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    RAW_DATA_DIR / \"training.1600000.processed.noemoticon.csv\",\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    encoding=\"latin-1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable: Positive sentences are labelled as 1\n",
    "df[\"positive\"] = df[\"sentiment\"].replace([0, 4], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df = df.drop(columns = [\"id\", \"query_string\", \"sentiment\", \"date\", \"user\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-29 14:00:41.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mAll sentences have been preprocessed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Process sentences\n",
    "# extracted from https://pub.aimind.so/a-comprehensive-guide-to-text-preprocessing-for-twitter-data-getting-ready-for-sentiment-analysis-e7f91cd03671\n",
    "\n",
    "# Lowercase\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Removing punctuation\n",
    "import string\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Removing numbers\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "# Removing extra spaces\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# Replacing repetitions of punctuation\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(r'(\\W)\\1+', r'\\1', x))\n",
    "\n",
    "# Removing special characters\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(r\"[^\\w\\s]\", '', x))\n",
    "\n",
    "# Remove contractions from the \"text\" column\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "logger.info(\"All sentences have been cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put target variable at the 1rst position\n",
    "df = df.reindex([\"positive\", \"text\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmarization, if it is indicated by the user\n",
    "with open(PARAMS_DIR, \"r\") as params_file:\n",
    "    try:\n",
    "        params = yaml.safe_load(params_file)\n",
    "        params = params[\"preprocessing\"]\n",
    "    except yaml.YAMLError as exc:\n",
    "            print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# POS tag mapping dictionary\n",
    "wordnet_map = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
    "\n",
    "# Function to perform Lemmatization on a text\n",
    "def lemmatize_text(text):\n",
    "    # Get the POS tags for the words\n",
    "    pos_tags = nltk.pos_tag(text)\n",
    "    \n",
    "    # Perform Lemmatization\n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos_tags:\n",
    "        # Map the POS tag to WordNet POS tag\n",
    "        pos = wordnet_map.get(tag[0].upper(), wordnet.NOUN)\n",
    "        # Lemmatize the word with the appropriate POS tag\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos=pos)\n",
    "        # Add the lemmatized word to the list\n",
    "        lemmatized_words.append(lemmatized_word)\n",
    "    \n",
    "    lemmatize_sentence = \" \".join(lemmatized_words)\n",
    "    return lemmatize_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"lemmarize\"] == True:\n",
    "\n",
    "    # Tokenize\n",
    "    try:\n",
    "        df['tokens'] = df[\"text\"].apply(lambda x: word_tokenize(x))\n",
    "    except LookupError: \n",
    "        import nltk\n",
    "        nltk.download('punkt_tab')\n",
    "        df['tokens'] = df[\"text\"].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "    try:\n",
    "        # Apply Lemmatization to the 'tokens' column\n",
    "        df['tokens'] = df[\"tokens\"].apply(lemmatize_text)\n",
    "    except LookupError:\n",
    "        nltk.download('averaged_perceptron_tagger_eng')\n",
    "        df['lemm_text'] = df[\"tokens\"].apply(lemmatize_text)\n",
    "\n",
    "    del df[\"tokens\"]\n",
    "    logger.info(\"Lemmarizer is appliyed to all sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "df.to_csv(str(PROCESSED_DATA_DIR) + \"/\" + \"preprocessed_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import PROCESSED_DATA_DIR, PARAMS_DIR\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    PROCESSED_DATA_DIR / \"preprocessed_dataset.csv\"\n",
    ")\n",
    "\n",
    "with open(PARAMS_DIR, \"r\") as params_file:\n",
    "    try:\n",
    "        params = yaml.safe_load(params_file)\n",
    "        params = params[\"split\"]\n",
    "    except yaml.YAMLError as exc:\n",
    "            print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"positive\"]\n",
    "X = df.drop(columns='positive')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=params[\"test_size\"], train_size = params[\"train_size\"], random_state = params[\"random_state\"])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size= params[\"val_size\"], train_size = 1 - params[\"val_size\"], random_state = params[\"random_state\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
